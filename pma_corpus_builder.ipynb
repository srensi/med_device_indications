{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMA Dataset\n",
    "\n",
    "This notebook contains code for retreiving PMA approval statements mentioning indications from OpenFDA, formatting them, and uploading to a collection on PubAnnotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Product Codes\n",
    "\n",
    "There are too many PMAs for us to search all at once, so we must group them by product code.  We first get a list of product codes (with PMA counts for each product code) from the pma endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://api.fda.gov/device/pma.json?count=product_code&limit=1000')\n",
    "response_json = response.json()\n",
    "product_codes = pandas.DataFrame(response_json['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Indication Statements\n",
    "\n",
    "For each product code we fetch PMA records where the keywords \"indicated\" or \"intended\" appear in the text of the approval statement summary.  We then generate ids from the pma number and supplement number, format summaries that are in ALL CAPS (this can break NER software), and store the results in a dictionary.  This is our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = 'ao_statement:indicated+ao_statement:intended'\n",
    "\n",
    "dataset = dict()\n",
    "for term in product_codes['term']:\n",
    "    url = f'https://api.fda.gov/device/pma.json?search=product_code:{term}+AND+({keywords})&limit=100'\n",
    "    response = requests.get(url)\n",
    "    response_json = response.json()\n",
    "    if 'results' not in response_json.keys():\n",
    "        continue\n",
    "    for result in response_json['results']:\n",
    "        pma_number = result['pma_number']\n",
    "        supplement_number = result['supplement_number']\n",
    "        if supplement_number == '':\n",
    "            supplement_number = 'S000'\n",
    "        pma_id = pma_number + '_' + supplement_number \n",
    "        statement = result['ao_statement']\n",
    "        if statement.isupper():\n",
    "            statement = '.  '.join([i.capitalize() for i in statement.split('.  ')])\n",
    "        dataset[pma_id] = statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to PubAnnotation\n",
    "\n",
    "Finally, we upload all of our documents to PubAnnotation for so we can annotate them and make them available to whoever would like to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_url = 'http://pubannotation.org/projects/blah6_medical_device/docs.json'\n",
    "auth=('srensi@stanford.edu', 'qukraw-7gumba-cEvdyf')\n",
    "payload = {\n",
    "    'sourceid' : '',\n",
    "    'text' : ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "for pma_id, pma_summary in dataset.items():\n",
    "    payload['sourceid'] = pma_id\n",
    "    payload['text'] = pma_summary\n",
    "    try:\n",
    "        r = requests.post(project_url, data=payload, auth=auth)\n",
    "    except:\n",
    "        failed.append(payload.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We have now uploaded a set of PMAs to PubAnnotation for to annotate and share with NLP researchers who are interested in using NER tools to annotate indications.  This is the first step in structuring the space of medical device indications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sourceid': 'P950021_S007',\n",
       "  'text': 'Approval for transfer of the assay to a new bayer platform, the advia centaur cp system.  The advia centaur cp psa assay is intended to quantitatively measure prostate-specific-antigen (psa) in human serum using the advia centaur cp system.'},\n",
       " {'sourceid': 'P930036_S003',\n",
       "  'text': 'Approval for the acs:180 and the centaur afp assays on the advia centaur cp system.  The device, as modified, will be marketed under the trade name advia centaur cp afp and is indicated for in vitro diagnostic use in the quantitative determination of alpha-fetoprotein (afp) in 1) human serum and in amniotic fluid from specimens obtained at 15 and 20 weeks gestation, as an aid in detecting open neural tube defects (ntds) when used in conjunction with ultrasonography and amniography testing 2) human serum, as an aid in managing non-seminomatous testicular cancer when used in conjunction with physical examination, histology/pathology, and other clinical evaluation procedures, using the (bayer) system.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
